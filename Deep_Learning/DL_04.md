# 다중 분류 - 뉴스 기사 분류



### 데이터 내용

- 로이터 뉴스를 46개의 상호 배타적 토픽으로 분류
- 로이터 데이터셋
  - 1986년 로이터에서 공개한 짧은 뉴스 기사와 토픽의 집합
  - 46개의 토픽
  - 각 토픽은 훈련 세트에 최소 10개의 sample을 가지고 있음



---



### 데이터 준비

```python
from keras.datasets import reuters

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
```

​	데이터셋을 train과 test로 나눈다.



```python
import numpy as np

def vectorize_sequences(seqs, dim=10000):
  results = np.zeros((len(seqs), dim))
  for i, seq in enumerate(seqs):
    results[i, seq] = 1.
  return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)


from keras.utils.np_utils import to_categorical

one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)
```

​	feature와 label 모두 벡터로 변환해 준다. 이때 label의 경우에는 one_hot encoding을 사용했다. one_hot encoding은 사전에 의해 단어 위치에 따라 데이터를 1로 변환하는 것이다. [0,0,1,0,0,1...0] 이런 식으로 말이다. 이렇게 해주면 메모리는 차지하겠지만 텐서플로우에서 처리가 더 수월해진다.



---



### 모델 구성

```python
from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
```

​	Dense 층을 쌓는다. 이때 이진 분류와의 차이점은 가장 마지막의 activation이 다른 건데, 이진 분류는 정답이 positive 혹은 nagative로 반환하기 때문에 sigmoid를 사용했다. 하지만 여기서는 class가 46개이기 때문에 softmax를 써준다. 그리고 중간층의 경우에는 마지막의 뉴런 개수보다 커야 한다. 그 이유는 뉴런의 개수가 줄다 보면 누락되는 데이터가 있어 성능이 좋지 않기 때문이다.



---



### 컴파일

```python
model.compile(optimizer = 'rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
```

​	모델을 컴파일 해준다.



---



### 훈련 검증

```python
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]

history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))
```

​	validation을 설정하여 과적합을 확인한다.



```python
import matplotlib.pyplot as plt

history_dict = history.history
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(loss)+1)

plt.figure(figsize=(16,12))
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.legend()
plt.show()
```

​	loss를 그래프로 확인한다.



```python
plt.clf()
acc=history_dict['accuracy']
val_acc = history_dict['val_accuracy']

plt.figure(figsize=(16,12))
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
```

​	accuracy를 그래프로 확인한다.



---



### 새로운 모델 훈련

```python
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64,activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(partial_x_train, partial_y_train, epochs=6, batch_size=512)
results = model.evaluate(x_test, one_hot_test_labels)
print(results)
```

​	과적합이 어디서 시작하는지 알았다면 새로운 모델을 만들어서 다시 훈련시켜준다. 새로운 모델을 만드는 이유는 기존에 훈련한 데이터에서 다시 훈련하게 되면 무용지물이기 때문이다.



---



### 새로운 데이터 예측

```python
>>> predictions = model.predict(x_test)
>>> predictions[0].shape
(46,)

>>> np.sum(predictions[0])
0.9999999

>>> np.argmax(predictions[0])
3
```

​	새로운 데이터를 예측해 보자.