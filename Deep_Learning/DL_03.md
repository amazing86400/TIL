# 이진 분류 - 영화 리뷰 분류



### 데이터 내용

- 리뷰 텍스트를 기반으로 영화리뷰를 긍정(positive)과 부정(negative)으로 분류
- 인터넷 영화 데이터베이스에서 가져온 양극단의 리뷰 5만개로 이루어 진 IMDB  데이터셋
- 훈련데이터 25000개와 테스트데이터 25000개
- 각각 50%는 긍정, 50%는 부정리뷰로 구성



---



### 데이터 준비

```python
from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)

print(train_data[0])
print(train_labels[0])
print(max([max(sequence) for sequence in train_data]))
```

​	데이터셋을 train과 test data로 나눈다. 그리고 train_data의 가장 첫 번째 데이터를 확인해보자.



```python
word_index = imdb.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])
print(decoded_review)
```

​	위 코드는 사전에 정의된 데이터를 가지고 가장 첫 번째 리뷰를 확인하는 코드다. 각 단어마다 숫자와 함께 key, value로 정의되어 있는데, 이것을 활용하여 확인하는 코드다.



```python
import numpy as np

def vectorize_sequences(seqs, dim=10000):
  results = np.zeros((len(seqs), dim))
  for i, seq in enumerate(seqs):
    results[i, seq] = 1.
  return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data) 

y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
```

​	정수 시퀀스를 이진 행렬로 인코딩한다. label에는 one_hot encoding을 하여 0과 1의 벡터로 변환한다. 각 단어가 정의된 숫자의 위치에 1로 표시된다. 원-핫 인코딩은 label에만 사용한다.



---



### 모델 구성

```python
from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```

​	Sequential() 함수로 층을 쌓는데, 은닉 유닛은 16개, 활성화 함수는 relu로 설정했다. Dense 층을 쌓을 때는 '얼마나 많은 층을 사용할지'와 '각 층에 얼마나 많은 은닉유닛을 사용할지'를 항상 고려해야한다. 그리고 마지막 층은 이진 분류이고 신경망의 출력이 확률이므로 은닉 유닛은 1, 활성화 함수는  sigmoid로 설정했다.



---



### 컴파일

```python
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
```

​	모델을 컴파일 해준다. optimizer는 'rmsprop'을 사용했는데, 이거는 웬만해서 거의 다 잘 작동한다. 그리고 loss는 'binary_crossentropy'로 지정해주었다. 확률을 출력하는 모델을 사용할 때는 크로스엔트로피가 최선의 선택이다.



---



### 훈련 검증

```python
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(partial_x_train, partial_y_train, epochs = 50, batch_size=512, validation_data=(x_val, y_val))


>>> history_dict = history.history
>>> history_dict.keys()
dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])
```

​	validation을 설정하여 훈련 데이터로 검증한다. 이때 validation은 train data가 아닌 데이터를 사용해야한다. 여기서는 train data에서 일부만 분리해서 사용했다. 이때 model.fit() 메서드는 History 객체를 반환한다. history 객체는 훈련하는 동안 발생한 모든 정보를 담고 있는 딕셔너리인 history 속성을 갖는다.



```python
import matplotlib.pyplot as plt

history_dict = history.history
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(loss)+1)

plt.figure(figsize=(16,12))
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label = 'Validation loss')
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.legend()
plt.show()


plt.clf()
acc = history_dict['acc']
val_acc = history_dict['val_acc']

plt.figure(figsize=(16,12))
plt.plot(epochs, acc, 'bo', label = 'Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
```

​	그래프를 통해 loss와 acc를 확인한다. 그냥 train data의 검증은 성능이 계속 향상되지만, 실제 validation을 확인하면 그렇지 않은 것을 확인할 수 있다.

​	validation을 통해 과적합이 시작된 부분을 확인한다.



---



### 새로운 모델 훈련

```python
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5, batch_size=512)
results = model.evaluate(x_test, y_test)
print(results)
```

​	과적합이 시작된 부분을 찾은 뒤, 새로운 모델을 만들어 최적의 모델을 훈련시킨다. 새로 모델을 만드는 이유는 이미 훈련된 데이터에서 다시 훈련시키면 무용지물이 되기 때문이다.
